# ğŸ§  LocalLLM-RAG

A self-hosted LLM system that answers queries exclusively from your own database â€” not the web. Built with a Retrieval-Augmented Generation (RAG) pipeline using PostgreSQL + pgvector, sentence-transformers, and Ollama.

![Python](https://img.shields.io/badge/Python-3.10+-blue?logo=python)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-14+-336791?logo=postgresql)
![Ollama](https://img.shields.io/badge/Ollama-Qwen_2.5_14B-orange)
![License](https://img.shields.io/badge/License-MIT-green)

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Query  â”‚â”€â”€â”€â”€â–¶â”‚  Embedding   â”‚â”€â”€â”€â”€â–¶â”‚  pgvector Similarity â”‚
â”‚              â”‚     â”‚  Model (GPU) â”‚     â”‚  Search (PostgreSQL) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                     â”‚
                                                     â–¼
                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                          â”‚  Top-K Relevant     â”‚
                                          â”‚  Documents/Reviews  â”‚
                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                     â”‚
                                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Response    â”‚â—€â”€â”€â”€â”€â”‚  Ollama LLM  â”‚â—€â”€â”€â”€â”€â”‚  Prompt + Context   â”‚
â”‚              â”‚     â”‚  (Qwen 2.5)  â”‚     â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Evaluation Results

| Metric | Score |
|--------|-------|
| **Retrieval Keyword Precision** | 100.0% |
| **Avg Similarity Score** | 0.7773 |
| **Retrieval Time** | 7.0ms |
| **Faithfulness** | 4.40/5 |
| **Relevance** | 4.80/5 |
| **Completeness** | 3.40/5 |
| **Hallucination Rate** | 20% |
| **Product Diversity** | 100.0% |
| **Avg End-to-End Latency** | 4.03s |
| **Tokens/sec** | 11.0 |

## ğŸ”§ Tech Stack

| Component          | Technology                          |
|--------------------|-------------------------------------|
| LLM                | Qwen 2.5 14B via Ollama             |
| Embedding Model    | all-MiniLM-L6-v2 (384 dims)        |
| Vector Database    | PostgreSQL 14 + pgvector (HNSW)     |
| Backend            | Python 3.10 + SQLAlchemy            |
| Web UI             | Gradio                              |
| GPU                | NVIDIA RTX 5070 Ti (12GB VRAM)      |
| Dataset            | Amazon Fine Food Reviews (568K)     |

## ğŸ“ Project Structure

```
LocalLLM-RAG/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ database/              # PostgreSQL connection, schema, queries
â”‚   â”‚   â”œâ”€â”€ connection.py      # DB engine & session management
â”‚   â”‚   â”œâ”€â”€ schema.py          # Table definitions & vector index
â”‚   â”‚   â””â”€â”€ queries.py         # SQL queries for retrieval
â”‚   â”œâ”€â”€ embeddings/            # Embedding generation & search
â”‚   â”‚   â”œâ”€â”€ generator.py       # Batch embedding with sentence-transformers
â”‚   â”‚   â””â”€â”€ search.py          # Vector similarity search
â”‚   â”œâ”€â”€ llm/                   # LLM interaction layer
â”‚   â”‚   â”œâ”€â”€ ollama_client.py   # Ollama API wrapper (streaming + sync)
â”‚   â”‚   â””â”€â”€ prompts.py         # Prompt templates for RAG & evaluation
â”‚   â”œâ”€â”€ rag/                   # RAG pipeline orchestration
â”‚   â”‚   â””â”€â”€ pipeline.py        # End-to-end retrieve â†’ generate pipeline
â”‚   â”œâ”€â”€ api/                   # Web interface
â”‚   â”‚   â””â”€â”€ app.py             # Gradio chat UI with sources panel
â”‚   â””â”€â”€ utils/                 # Shared utilities
â”‚       â””â”€â”€ config.py          # Configuration loader from .env
â”œâ”€â”€ config/                    # Configuration files
â”œâ”€â”€ notebooks/                 # Jupyter notebooks (step-by-step)
â”‚   â”œâ”€â”€ 00_Creating_PostgreSQL_DB.ipynb
â”‚   â”œâ”€â”€ 01_embedding.ipynb
â”‚   â”œâ”€â”€ 02_rag_pipeline.ipynb
â”‚   â””â”€â”€ 03_evaluation.ipynb
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_pipeline.py        # CLI entry point
â”‚   â””â”€â”€ start.sh               # Quick launch script
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Original Kaggle dataset
â”‚   â””â”€â”€ processed/             # Cleaned data
â”œâ”€â”€ tests/                     # Test suite
â”œâ”€â”€ docs/                      # Documentation
â”œâ”€â”€ .env.example               # Environment variable template
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### Prerequisites

- Ubuntu 22.04+
- NVIDIA GPU with 12GB+ VRAM (CUDA support)
- Python 3.10+
- PostgreSQL 14+ with pgvector extension
- Ollama

### 1. Clone & Setup

```bash
git clone https://github.com/maliciit-sys/LocalLLM-RAG.git
cd LocalLLM-RAG
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 2. Configure Environment

```bash
cp .env.example .env
nano .env
# Fill in your database credentials and model preferences
```

### 3. Setup PostgreSQL + pgvector

```bash
sudo apt install postgresql postgresql-contrib -y
cd /tmp && git clone --branch v0.8.0 https://github.com/pgvector/pgvector.git
cd pgvector && make && sudo make install

sudo -u postgres psql -c "CREATE USER llmuser WITH PASSWORD 'your_password';"
sudo -u postgres psql -c "CREATE DATABASE llmdb OWNER llmuser;"
sudo -u postgres psql -d llmdb -c "CREATE EXTENSION vector;"
```

### 4. Download Dataset

Download [Amazon Fine Food Reviews](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews) from Kaggle and place `Reviews.csv` in `data/raw/`.

### 5. Load Data & Generate Embeddings

Run the notebooks in order:
1. `notebooks/00_Creating_PostgreSQL_DB.ipynb` â€” Load CSV into PostgreSQL
2. `notebooks/01_embedding.ipynb` â€” Generate vector embeddings on GPU

### 6. Install LLM

```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama pull qwen2.5:14b
```

### 7. Launch

```bash
# Web UI
python scripts/run_pipeline.py --serve

# Terminal chat
python scripts/run_pipeline.py --chat

# Single query
python scripts/run_pipeline.py --query "What do people think about organic coffee?"

# Database stats
python scripts/run_pipeline.py --stats
```

Open `http://localhost:7860` for the web interface.

## ğŸ–¥ï¸ Web UI Features

- ğŸ’¬ **Streaming chat** â€” real-time token-by-token responses
- ğŸ“„ **Sources panel** â€” shows retrieved reviews with similarity scores
- âš™ï¸ **Adjustable settings** â€” Top-K and temperature controls
- ğŸ’¡ **Example queries** â€” pre-built questions to try
- ğŸ“Š **Database stats** â€” live data overview

## ğŸ§ª Example Queries

```
> What do people think about organic coffee? Is it worth buying?
> Which dog food products have the best reviews and why?
> What are common complaints about chocolate products?
> Are there any highly rated gluten-free snacks?
> What's the best tea according to reviewers?
> Do people like sugar-free candy?
```

## ğŸ”¬ How RAG Works

1. **Embed**: Your question is converted to a 384-dimensional vector using `all-MiniLM-L6-v2`
2. **Retrieve**: pgvector HNSW index finds the Top-K most similar reviews (~7ms)
3. **Augment**: Retrieved reviews are injected into the prompt as context
4. **Generate**: Qwen 2.5 14B generates an answer grounded ONLY in retrieved reviews

All processing happens **locally on your machine**. No data is sent to any external service.

## ğŸ’» Hardware Used

| Component | Specification |
|-----------|--------------|
| Laptop | Lenovo Legion 5 Pro |
| RAM | 32 GB |
| Storage | 2 TB NVMe |
| GPU | NVIDIA RTX 5070 Ti (12GB VRAM) |
| OS | Ubuntu 22.04 |

## ğŸ“ˆ Future Improvements

- [ ] Add re-ranking after retrieval (cross-encoder)
- [ ] Upgrade to larger embedding model (bge-large-en-v1.5)
- [ ] Implement hybrid search (semantic + keyword)
- [ ] Add conversation memory with context windowing
- [ ] Support multiple datasets
- [ ] Docker containerization

## ğŸ“„ License

MIT License

## ğŸ¤ Acknowledgments

- [Ollama](https://ollama.com/) â€” local LLM serving
- [pgvector](https://github.com/pgvector/pgvector) â€” vector similarity for PostgreSQL
- [Sentence Transformers](https://www.sbert.net/) â€” embedding models
- [Amazon Fine Food Reviews](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews) â€” dataset
- [Gradio](https://gradio.app/) â€” web UI framework
